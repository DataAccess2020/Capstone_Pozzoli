---
title: "The Italin parties' debate on the most relevant issues of last months"
subtitle: "An analysys of the Twitter debate on the war in Ukraine, the protests in Iran, the energetic crisis and the Qatar-gate"
author: "Pozzoli Cristina"
date: "12/23/2022"
output: html_document
---

```{r setup, include=FALSE, eval= F}
knitr::opts_chunk$set(echo = T)
```

Link to Capstone_Pozzoli repository: <https://github.com/DataAccess2020/Capstone_Pozzoli.git>

Number of commits: 58

Number of pull requests: 4

## **Introduction**

The last period has been marked by some events that have considerably changed some equilibria and effected our lives and that have ignited the public debate. First of all, the war in Ukraine, the first war on European soil after 70 years, which has led to the most relevant energetic crisis since the one in the 70s. Second of all, the protests in Iran against the regime, which have shown that in some countries the violation of human rights and the death penalty for the resistance to the regime are a daily occurrence. And thirdly, the recent Qatar-gate, an episode of corruption inside the European Parliament, that was a shock for all the European citizens.

This research aims to analyze the Twitter accounts of the main Italian parties, in order to see how many tweets they have published concerning the four topics listed above. More precisely, in the research are taken into account their tweets of the last few months.

The goal is to understand if the parties focused their attention on those that we can define as the most relevant events of the recent period, or if they focused their attention on other topics.

## **Methodology**

To pursue this objective, I selected only the Italian parties that have obtained at least 15 seats in the Camera dei Deputati. Then, I have downloaded and analyzed their tweets using some keywords for each topic, in order to see how frequently the topic emerged. To show the differences between parties, I have created some plots for the absolute frequency distribution and then a plot for the relative frequency distribution. At the end, I have also created some word clouds to see which are the most frequent words in each party's tweets.

### **Twitter lists**

In order to start my analysis I have created seven lists on my twitter account. Each list contains the twitter accounts of the party and the corresponding party leader. I report the links to lists below:

-   Azione: <https://twitter.com/i/lists/1603693345603260416?s=20>
-   Fratelli d'Italia : <https://twitter.com/i/lists/1603694183209902080?s=20>
-   Forza Italia: <https://twitter.com/i/lists/1603693826442412032?s=20>
-   Italia Viva: <https://twitter.com/i/lists/1603691179392081922?s=20>
-   Lega Salvini Premier: <https://twitter.com/i/lists/1603691986539659265?s=20>
-   Movimento Cinque Stelle: <https://twitter.com/i/lists/1603692787651825664?s=20>
-   Partito Democratico: <https://twitter.com/i/lists/1603692493324910594?s=20>

### **Authentication and lists import**

The first step to obtain data from the lists is to get a Twitter API. After authenticating to the 'Essential' Twitter API, I imported my lists form Twitter, creating a data frame for each of them with the function 'list_members' from the 'rtweet' package.

```{r,eval=F, warning=FALSE, message=FALSE}
#Authentication 

library(rtweet)
library(tidyverse)
library(jsonlite)

auth_setup_default()

# Lists import 

italia_viva = lists_members(
  list_id = "1603691179392081922",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

lega = lists_members(
  list_id = "1603691986539659265",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

Partito_democratico = lists_members(
  list_id = "1603692493324910594",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

Movimento_5S = lists_members(
  list_id = "1603692787651825664",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

Azione = lists_members(
  list_id = "1603693345603260416",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

Forza_Italia = lists_members(
  list_id = "1603693826442412032",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)

Fratelli_dItalia = lists_members(
  list_id = "1603694183209902080",
  slug = NULL,
  owner_user = NULL,
  n = 2,
  cursor = "-1",
  token = NULL,
  retryonratelimit = TRUE,
  verbose = TRUE,
  parse = TRUE,
)
```

### Tweets download

To download the tweets I used the function 'get_timeline' form the 'rtweet' package. Then, I have created a data frame for each party, which contains 43 variables, including the tweets' full text and the creation date.

```{r, warning=FALSE, message=FALSE, eval=F}
tweets_Azione <- get_timeline(user = Azione$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)



tweets_FI <- get_timeline(user = Forza_Italia$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)



tweets_lega <- get_timeline(user = lega$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)



tweets_M5S <- get_timeline(user = Movimento_5S$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)



tweets_PD <- get_timeline(user = Partito_democratico$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)



tweets_FDI <- get_timeline(user = Fratelli_dItalia$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)




tweets_IV <- get_timeline(user = italia_viva$screen_name,
                              n= 10000,
                              verbose = T,
                              parse = T,
                              Sys.sleep(0.5)
)
```

After doing that, I have saved the results inside the folder 'Output_Dataframe' using the function 'sapply()' and the function 'write.csv()'.

```{r, warning=FALSE, message=FALSE, eval=F}

tweets_Azione2 <- sapply(tweets_Azione, as.character)
write.csv(tweets_Azione2, "Azione_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_FDI2 <- sapply(tweets_FDI, as.character)
write.csv(tweets_FDI2, "FDI_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_FI2 <- sapply(tweets_FI, as.character)
write.csv(tweets_FI2, "FI_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_IV2 <- sapply(tweets_IV, as.character)
write.csv(tweets_IV2, "IV_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_lega2 <- sapply(tweets_lega, as.character)
write.csv(tweets_lega2, "lega_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_M5S2 <- sapply(tweets_M5S, as.character)
write.csv(tweets_M5S2, "M5S_tweets.csv", row.names = F, fileEncoding = "UTF-8")

tweets_PD2 <- sapply(tweets_PD, as.character)
write.csv(tweets_PD2, "PD_tweets.csv", row.names = F, fileEncoding = "UTF-8")

```

## **Text Analysis**

To analyse the frequency of tweets for each topic, I have created some regular expressions, containing the keywords for each topic. Then, I have computed the frequency of the keywords on tweets and I have created a data frame containing this information.

```{r, echo=TRUE, warning=FALSE, message=FALSE, eval=F}
#Regular expressions 

regex_iran<- "([I|i]ran)|([M|m]ahsa [A|a]mini)"
regex_ukr <- "([U|u]craina)|([Z|z]elensky)|([K|k]iev)|([P|p]utin)"
regex_qatar <- "([Q|q]atar gate)|([K|k]aili)|([P|p]anzeri)|([G|g]iorgi)"
regex_ce <- "(caro energia)|(caro bollette)|(prezzo del gas)|(transizione energetica)"

#Analysis

sum(str_count(tweets_Azione$full_text, pattern = regex_iran)) 
sum(str_count(tweets_Azione$full_text, pattern = regex_ukr))
sum(str_count(tweets_Azione$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_Azione$full_text, pattern = regex_ce)) 

sum(str_count(tweets_FDI$full_text, pattern = regex_iran)) 
sum(str_count(tweets_FDI$full_text, pattern = regex_ukr))
sum(str_count(tweets_FDI$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_FDI$full_text, pattern = regex_ce))

sum(str_count(tweets_FI$full_text, pattern = regex_iran)) 
sum(str_count(tweets_FI$full_text, pattern = regex_ukr))
sum(str_count(tweets_FI$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_FI$full_text, pattern = regex_ce))

sum(str_count(tweets_IV$full_text, pattern = regex_iran)) 
sum(str_count(tweets_IV$full_text, pattern = regex_ukr))
sum(str_count(tweets_IV$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_IV$full_text, pattern = regex_ce))

sum(str_count(tweets_lega$full_text, pattern = regex_iran)) 
sum(str_count(tweets_lega$full_text, pattern = regex_ukr))
sum(str_count(tweets_lega$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_lega$full_text, pattern = regex_ce))

sum(str_count(tweets_M5S$full_text, pattern = regex_iran)) 
sum(str_count(tweets_M5S$full_text, pattern = regex_ukr))
sum(str_count(tweets_M5S$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_M5S$full_text, pattern = regex_ce))

sum(str_count(tweets_PD$full_text, pattern = regex_iran)) 
sum(str_count(tweets_PD$full_text, pattern = regex_ukr))
sum(str_count(tweets_PD$full_text, pattern = regex_qatar)) 
sum(str_count(tweets_PD$full_text, pattern = regex_ce))

#Data frame

Parties <- c ("Azione", "Fratelli d'Italia", "Forza Italia", "Italia Viva", 
              "Lega", "Movimento 5 Stelle", "Partito democratico")

tweets_iran <- c("47", "80", "31", "34", "23", "27", "47")

tweets_ucraina <- c("259", "67", "119", "116", "39", "57", "594")

tweets_qatar <- c("155", "668", "161", "81", "40", "83", "131")

tweets_energia <- c("28", "27", "21", "15", "36", "37", "27")

tweets_frequency <- data.frame(Parties, tweets_iran, 
                               tweets_ucraina, tweets_qatar, tweets_energia)


write.csv(tweets_frequency, "tweets_frequency.csv", 
          row.names = F, fileEncoding = "UTF-8")

```

### **Plots**

Now that I have the frequency distribution, I can create some plots to show the differences between parties. The first four plots show only the absolute frequency distribution and they are useful to see which party has written more about a certain topic.

In order to see which is the most common of the four topics, I decided to compute also the relative frequencies and to put the four plots together in the same image.

```{r, echo=F, warning=FALSE, message=FALSE, eval=F}
library(ggplot2)
library(scales)

tweets_frequency %>% 
  ggplot(aes(x = Parties, y=tweets_iran)) +
  geom_col(alpha=0.8, fill="pink") +
  ylab("Tweets about Iran") +
  xlab("Parties") +
  ggtitle("Frequency distribution of tweets about Iran for each party") +
  scale_x_discrete(labels = label_wrap(10))  +
  theme_bw()

tweets_frequency %>% 
  ggplot(aes(x = Parties, y=tweets_ucraina)) +
  geom_col(alpha=0.5, fill="blue") +
  ylab("Tweets about Ukraine") +
  xlab("Parties") +
  ggtitle("Frequency distribution of tweets about Ukraine for each party") +
  scale_x_discrete(labels = label_wrap(10))  +
  theme_bw()

tweets_frequency %>% 
  ggplot(aes(x = Parties, y=tweets_qatar)) +
  geom_col(alpha=0.8, fill="green") +
  ylab("Tweets about Qatar") +
  xlab("Parties") +
  ggtitle("Frequency distribution of tweets about Qatar for each party") +
  scale_x_discrete(labels = label_wrap(10))  + 
  theme_bw()

tweets_frequency %>% 
  ggplot(aes(x = Parties, y=tweets_energia)) +
  geom_col(alpha=0.8, fill="red") +
  ylab("Tweets about Energetic crisis") +
  xlab("Parties") +
  ggtitle("Frequency distribution of tweets about Energy crisis for each party") +
  scale_x_discrete(labels = label_wrap(10)) + 
  theme_bw()

#Comparison 
library(ggpubr)
a <- tweets_frequency %>% 
  ggplot(aes(x = Parties, y=(tweets_iran)/sum(tweets_iran + tweets_ucraina 
                                              + tweets_qatar + tweets_energia)))+
  geom_col(alpha=0.8, fill="pink") +
  ylab("Tweets about Iran") +
  xlab("Parties") +
  scale_y_continuous(labels = scales::percent)+
  scale_x_discrete(labels = label_wrap(10)) +
  theme_bw()
b <- tweets_frequency %>% 
  ggplot(aes(x = Parties, y=(tweets_ucraina)/sum(tweets_iran + tweets_ucraina 
                                                 + tweets_qatar + tweets_energia)))+
  geom_col(alpha=0.5, fill="blue") +
  ylab("Tweets about Ukraine") +
  xlab("Parties") +
  scale_y_continuous(labels = scales::percent)+
  scale_x_discrete(labels = label_wrap(10))  +
  theme_bw()
c <-tweets_frequency %>% 
  ggplot(aes(x = Parties, y=(tweets_qatar)/sum(tweets_iran + tweets_ucraina 
                                               + tweets_qatar + tweets_energia))) +
  geom_col(alpha=0.5, fill="green") +
  ylab("Tweets about Qatar") +
  xlab("Parties") +
  scale_x_discrete(labels = label_wrap(10))  + 
  scale_y_continuous(labels = scales::percent) +
  theme_bw()
d <- tweets_frequency %>% 
  ggplot(aes(x = Parties, y=(tweets_energia)/sum(tweets_iran + tweets_ucraina 
                                                 + tweets_qatar + tweets_energia))) +
  geom_col(alpha=0.5, fill="red") +
  ylab("Tweets about Energetic crisis") +
  xlab("Parties") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_discrete(labels = label_wrap(10)) + 
  theme_bw()
ggarrange(a, b, c, d, 
          ncol = 2, nrow = 2) +
  ggsave("Finalplot_for_comparison.jpeg", width = 25, height = 20, units = "cm")


```

![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Plots/Iran_tweets_distribution.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Plots/Ukraine_tweets_distribution.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Plots/Qatar_tweets_distribution.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Plots/Energycrisis_tweets_distribution.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Plots/Finalplot_for_comparison.jpeg)
### **Word clouds**

The last passage of the analysis consists on the creation of some word clouds. The word clouds are useful to understand which are the most common topics on the Twitter parties' accounts. Before creating the word clouds it was necessary to clean the text from punctuation, numbers, hashes and other useless staff. To create the word clouds I used the function 'wordcloud' form the homonymous package.

```{r, eval=F, warning=FALSE, message=FALSE}
library(RColorBrewer)
library(tm)
library(stopwords)
library(tidytext)
library(stringr)

#Text cleaning 

Azione <- as.character(tweets_Azione$full_text)
Azione<- str_replace_all(tweets_Azione$full_text, "[\'’]", "' ")
Azione<- gsub("\\$", "", tweets_Azione$full_text) 
Azione <- gsub("@\\w+", "",tweets_Azione$full_text)
Azione <- gsub("[[:punct:]]","", tweets_Azione$full_text)
Azione <- gsub("http\\w+", "", tweets_Azione$full_text)
Azione <- gsub("[ |\t]{2,}", "", tweets_Azione$full_text)
Azione <- gsub("^ ", "",tweets_Azione$full_text)
Azione <- gsub(" $", "", tweets_Azione$full_text)
Azione <- gsub("href", "", tweets_Azione$full_text)
Azione <- gsub("([0-9])","", tweets_Azione$full_text)

stop_words_ita<-tibble(word=stopwords("it"))

tokens_Azione <- tibble(text = Azione) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_Azione_save<-sapply(tokens_Azione, as.character)

clean_Azione <- Corpus(VectorSource(tokens_Azione_save))

clean_Azione <- clean_Azione %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_Azione <- tm_map(clean_Azione, content_transformer(tolower))
clean_Azione <- tm_map(clean_Azione, removeWords, stopwords("italian"))

matrix_Azione <- TermDocumentMatrix(clean_Azione) 
matrix_Azione <- as.matrix(matrix_Azione) 
words <- sort(rowSums(matrix_Azione),decreasing=TRUE) 
df_Azione <- data.frame(word = names(words),freq=words) 

# word clouds 

library(wordcloud2)
library(wordcloud)

wordcloud(words = df_Azione$word, freq = df_Azione$freq, min.freq = 100,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))

# Repeat for each party 

FDI <- as.character(tweets_FDI$full_text)
FDI<- str_replace_all(tweets_FDI$full_text, "[\'’]", "' ")
FDI<- gsub("\\$", "", tweets_FDI$full_text) 
FDI <- gsub("@\\w+", "", tweets_FDI$full_text)
FDI <- gsub("[[:punct:]]","", tweets_FDI$full_text)
FDI <- gsub("http\\w+", "", tweets_FDI$full_text)
FDI <- gsub("[ |\t]{2,}", "", tweets_FDI$full_text)
FDI <- gsub("^ ", "", tweets_FDI$full_text)
FDI <- gsub(" $", "", tweets_FDI$full_text)
FDI <- gsub("href", "", tweets_FDI$full_text)
FDI <- gsub("([0-9])","", tweets_FDI$full_text)

tokens_FDI <- tibble(text = FDI) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_FDI_save<-sapply(tokens_FDI, as.character)

clean_FDI <- Corpus(VectorSource(tokens_FDI_save))

clean_FDI <- clean_FDI %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_FDI <- tm_map(clean_FDI, content_transformer(tolower))
clean_FDI <- tm_map(clean_FDI, removeWords, stopwords("italian"))

matrix_FDI <- TermDocumentMatrix(clean_FDI) 
matrix_FDI <- as.matrix(matrix_FDI) 
words <- sort(rowSums(matrix_FDI),decreasing=TRUE) 
df_FDI <- data.frame(word = names(words),freq=words) 

wordcloud(words = df_FDI$word, freq = df_FDI$freq, min.freq = 100,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))


FI <- as.character(tweets_FI$full_text)
FI<- str_replace_all(tweets_FI$full_text, "[\'’]", "' ")
FI<- gsub("\\$", "", tweets_FI$full_text) 
FI <- gsub("@\\w+", "", tweets_FI$full_text)
FI <- gsub("[[:punct:]]","", tweets_FI$full_text)
FI <- gsub("http\\w+", "", tweets_FI$full_text)
FI <- gsub("[ |\t]{2,}", "", tweets_FI$full_text)
FI<- gsub("^ ", "", tweets_FI$full_text)
FI <- gsub(" $", "", tweets_FI$full_text)
FI <- gsub("href", "", tweets_FI$full_text)
FI <- gsub("([0-9])","", tweets_FI$full_text)

tokens_FI <- tibble(text = FI) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_FI_save<-sapply(tokens_FI, as.character)

clean_FI <- Corpus(VectorSource(tokens_FI_save))

clean_FI <- clean_FI %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_FI <- tm_map(clean_FI, content_transformer(tolower))
clean_FI <- tm_map(clean_FI, removeWords, stopwords("italian"))

matrix_FI <- TermDocumentMatrix(clean_FI) 
matrix_FI <- as.matrix(matrix_FI) 
words <- sort(rowSums(matrix_FI),decreasing=TRUE) 
df_FI <- data.frame(word = names(words),freq=words) 

wordcloud(words = df_FI$word, freq = df_FI$freq, min.freq = 100,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))


IV <- as.character(tweets_IV$full_text)
IV<- str_replace_all(tweets_IV$full_text, "[\'’]", "' ")
IV<- gsub("\\$", "", tweets_IV$full_text) 
IV <- gsub("@\\w+", "",tweets_IV$full_text)
IV <- gsub("[[:punct:]]","",tweets_IV$full_text)
IV <- gsub("http\\w+", "", tweets_IV$full_text)
IV <- gsub("[ |\t]{2,}", "", tweets_IV$full_text)
IV<- gsub("^ ", "", tweets_IV$full_text)
IV <- gsub(" $", "", tweets_IV$full_text)
IV <- gsub("href", "", tweets_IV$full_text)
IV <- gsub("([0-9])","", tweets_IV$full_text)

tokens_IV <- tibble(text = IV) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_IV_save<-sapply(tokens_IV, as.character)

clean_IV <- Corpus(VectorSource(tokens_IV_save))

clean_IV <- clean_IV %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_IV <- tm_map(clean_IV, content_transformer(tolower))
clean_IV <- tm_map(clean_IV, removeWords, stopwords("italian"))

matrix_IV <- TermDocumentMatrix(clean_IV) 
matrix_IV <- as.matrix(matrix_IV) 
words <- sort(rowSums(matrix_IV),decreasing=TRUE) 
df_IV <- data.frame(word = names(words),freq=words) 

wordcloud(words = df_IV$word, freq = df_IV$freq, min.freq = 100,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))


lega <- as.character(tweets_lega$full_text)
lega <- str_replace_all(tweets_lega$full_text, "[\'’]", "' ")
lega <- gsub("\\$", "", tweets_lega$full_text) 
lega <- gsub("@\\w+", "", tweets_lega$full_text)
lega <- gsub("[[:punct:]]","", tweets_lega$full_text)
lega <- gsub("http\\w+", "", tweets_lega$full_text)
lega <- gsub("[ |\t]{2,}", "", tweets_lega$full_text)
lega <- gsub("^ ", "", tweets_lega$full_text)
lega <- gsub(" $", "",tweets_lega$full_text)
lega <- gsub("href", "", tweets_lega$full_text)
lega <- gsub("([0-9])","", tweets_lega$full_text)

tokens_lega <- tibble(text = lega) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_lega_save<-sapply(tokens_lega, as.character)

clean_lega <- Corpus(VectorSource(tokens_lega_save))

clean_lega <- clean_lega %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_lega <- tm_map(clean_lega, content_transformer(tolower))
clean_lega <- tm_map(clean_lega, removeWords, stopwords("italian"))

matrix_lega <- TermDocumentMatrix(clean_lega) 
matrix_lega <- as.matrix(matrix_lega) 
words <- sort(rowSums(matrix_lega),decreasing=TRUE) 
df_lega<- data.frame(word = names(words),freq=words) 

wordcloud(words = df_lega$word, freq = df_lega$freq, min.freq = 100,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))


M5S <- as.character(tweets_M5S$full_text)
M5S<- str_replace_all(tweets_M5S$full_text, "[\'’]", "' ")
M5S<- gsub("\\$", "", tweets_M5S$full_text) 
M5S <- gsub("@\\w+", "", tweets_M5S$full_text)
M5S<- gsub("[[:punct:]]","", tweets_M5S$full_text)
M5S <- gsub("http\\w+", "", tweets_M5S$full_text)
M5S <- gsub("[ |\t]{2,}", "", tweets_M5S$full_text)
M5S <- gsub("^ ", "", tweets_M5S$full_text)
M5S <- gsub(" $", "", tweets_M5S$full_text)
M5S <- gsub("href", "",tweets_M5S$full_text)
M5S <- gsub("([0-9])","", tweets_M5S$full_text)

tokens_M5S <- tibble(text = M5S) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_M5S_save<-sapply(tokens_M5S, as.character)

clean_M5S <- Corpus(VectorSource(tokens_M5S_save))

clean_M5S<- clean_M5S %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_M5S <- tm_map(clean_M5S, content_transformer(tolower))
clean_M5S <- tm_map(clean_M5S, removeWords, stopwords("italian"))

matrix_M5S <- TermDocumentMatrix(clean_M5S) 
matrix_M5S <- as.matrix(matrix_M5S) 
words <- sort(rowSums(matrix_M5S),decreasing=TRUE) 
df_M5S <- data.frame(word = names(words),freq=words) 

wordcloud(words = df_M5S$word, freq = df_M5S$freq, min.freq = 80,
          max.words=120, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))


PD <- as.character(tweets_PD$full_text)
PD<- str_replace_all(tweets_PD$full_text, "[\'’]", "' ")
PD<- gsub("\\$", "", tweets_PD$full_text) 
PD <- gsub("@\\w+", "", tweets_PD$full_text)
PD <- gsub("[[:punct:]]","", tweets_PD$full_text)
PD <- gsub("http\\w+", "", tweets_PD$full_text)
PD <- gsub("[ |\t]{2,}", "", tweets_PD$full_text)
PD<- gsub("^ ", "", tweets_PD$full_text)
PD <- gsub(" $", "", tweets_PD$full_text)
PD <- gsub("href", "", tweets_PD$full_text)
PD <- gsub("([0-9])","", tweets_PD$full_text)

tokens_PD <- tibble(text = PD) %>%
  unnest_tokens(word, text) %>%
  dplyr::anti_join(stop_words_ita)%>%
  count(word, sort = TRUE)

tokens_PD_save<-sapply(tokens_PD, as.character)

clean_PD <- Corpus(VectorSource(tokens_PD_save))

clean_PD <- clean_PD %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
clean_PD <- tm_map(clean_PD, content_transformer(tolower))
clean_PD <- tm_map(clean_PD, removeWords, stopwords("italian"))

matrix_PD <- TermDocumentMatrix(clean_PD) 
matrix_PD <- as.matrix(matrix_PD) 
words <- sort(rowSums(matrix_PD),decreasing=TRUE) 
df_PD <- data.frame(word = names(words),freq=words) 

wordcloud(words = df_PD$word, freq = df_PD$freq, min.freq = 90,
          max.words=120, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"),
          scale=c(2,0.60))
```

![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /Azione.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /FDI.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /FI.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /IV.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /lega.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /M5S.jpeg)
![](/Users/cristinapozzoli/Desktop/Capstone_Pozzoli/Word_clouds /PD.jpeg)


## **Conclusions**

From the results of this research emerged that, in absolute terms, Fratelli d'Italia is the party that tweeted more on Quatar and Iran issues, while the Partito Democratico was the one to publish more tweets on the war in Ukraine.

It is interesting to see that the tweets on the energetic crisis are almost equally distributed. A possible explanation could be linked to the fact that this topic is a domestic policy issue, so each party pay the same attention on it .

By looking at the final plot, in which we find the relative frequencies, we can see that both the tweets on the war in Ukraine and the tweets on the Qatar-gate are more than 20% of total tweets.

By looking at the word clouds we see that the only keyword, form the ones selected before, that emerged in each word cloud, is the word 'ucraina', so we can conclude that all the parties focus their attention on the war in Ukraine. While we can find none of the other keywords inside the word clouds. Instead, the most frequent words that we can find in each word cloud are 'italia', 'immigrazione', 'emergenza' and 'europa'.




